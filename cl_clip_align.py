# -*- coding: utf-8 -*-
"""CL_Clip_Align.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEYjvRO1vl6aHY9gqfd6ezEaHQGRV94P

This is simplified code of paper "Cross-Lingual Representation Alignment Through Contrasive Image-Caption Tuning".

paper link : https://arxiv.org/abs/2505.13628

paper git : https://github.com/nkrasner/cl-clip-align (not provided yet)

"""

from google.colab import drive
import os
import json
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torchvision.transforms import ToTensor
from tqdm import tqdm
from PIL import Image
import torch



"""# Using nn.module

##7. Training
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import XLMRobertaModel, XLMRobertaTokenizer, ViTModel, ViTImageProcessor
#Train with all languages
# TRAIN THIS CODE!!!!
from torch.utils.data import DataLoader
from torchvision import transforms
import matplotlib.pyplot as plt
from IPython.display import clear_output, display

image_preprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')
num_data = 15000 # train할 이미지 개수

# 데이터셋 & 전처리
lang_list = ['en', 'ko', 'es', 'fr', 'de', 'ru', 'zh', 'ja', 'th', 'sw', 'bn']

translated_captions_json_path = DRIVE_PATH + "/train2017/captions_train2017_translated.json" # 15000개 잇음
save_path = f'/content/drive/MyDrive/CS371/Final Research/{num_data}model.pth'

# 데이터셋 생성 (transform 제거)
dataset = MultilingualCocoDataset(
    captions_json_path=translated_captions_json_path,
    img_dir=save_dir,
    lang_list=lang_list,
    num_data = num_data
)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


class MultimodalCLIP(nn.Module):
    def __init__(self, device):
        super().__init__()
        self.device = device

        # Text components
        self.text_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')
        self.text_encoder = XLMRobertaModel.from_pretrained('xlm-roberta-large')

        # Image components
        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
        self.image_preprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')

        # Projection layers
        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, 512)
        self.image_proj = nn.Linear(self.image_encoder.config.hidden_size, 512)

        # Temperature parameter - this will be properly registered
        self.log_temperature = nn.Parameter(torch.tensor(0.07).log())

    def encode_text(self, texts):
        encoded = self.text_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(self.device)
        output = self.text_encoder(**encoded)
        last_hidden = output.last_hidden_state
        mask = encoded.attention_mask.unsqueeze(-1)
        pooled = (last_hidden * mask).sum(1) / mask.sum(1)
        return self.text_proj(pooled)

    def encode_image(self, images):
        from torchvision import transforms
        processed_images = []
        for img_tensor in images:
            img_pil = transforms.ToPILImage()(img_tensor)
            processed_images.append(img_pil)

        inputs = self.image_preprocessor(images=processed_images, return_tensors="pt").to(self.device)
        outputs = self.image_encoder(**inputs)
        cls_tokens = outputs.last_hidden_state[:, 0]
        return self.image_proj(cls_tokens)

    def contrastive_loss(self, image_embeds, text_embeds):
        temperature = torch.exp(self.log_temperature)
        image_embeds = F.normalize(image_embeds, dim=1)
        text_embeds = F.normalize(text_embeds, dim=1)
        logits = torch.matmul(text_embeds, image_embeds.T) / temperature
        targets = torch.arange(len(image_embeds)).to(image_embeds.device)
        loss_i2t = F.cross_entropy(logits, targets)
        loss_t2i = F.cross_entropy(logits.T, targets)
        return (loss_i2t + loss_t2i) / 2

# Usage
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultimodalCLIP(device).to(device)

# Training setup
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

# Freeze encoders initially
model.text_encoder.eval()
model.image_encoder.eval()
for param in model.text_encoder.parameters():
    param.requires_grad = False
for param in model.image_encoder.parameters():
    param.requires_grad = False

# Training loop
num_epochs = 10
thaw_epoch = 1
ind = 0
epoch_losses = []
iteration_losses = []
display_interval = 30  # 30 iteration마다 시각화

# if os.path.exists(save_path):
#     print(f"Loading saved model parameters from {save_path} ...")
#     print("Assuming warming up is already done, skip thaw epoch")
#     model.load_state_dict(torch.load(save_path))
#     model.to(device)
#     # 이미 저장된 파라미터가 있으면 thaw 단계 건너뜀
#     thaw_epoch = num_epochs  # thaw_epoch를 epoch 범위 밖으로 설정해서 thaw 무시
# else:
#     print("No saved model parameters found, training from scratch.")

for epoch in range(num_epochs):
    if epoch == thaw_epoch:
        print("Thaw epoch finished. Start to fine-tune pre-trained encoders")
        # Unfreeze encoders
        model.text_encoder.train()
        model.image_encoder.train()
        for param in model.text_encoder.parameters():
            param.requires_grad = True
        for param in model.image_encoder.parameters():
            param.requires_grad = True

        # Recreate optimizer to include all parameters
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

    running_loss = 0.0
    batch_count = 0

    for i, (images, captions_dict) in enumerate(tqdm(dataloader)):
        optimizer.zero_grad()

        # Your existing data processing code...
        # normalized_images = []
        # for img in images:
        #     normalized_img = dataset.normalize(img)
        #     normalized_images.append(normalized_img)

        lang = lang_list[ind]
        ind = (ind + 1) % len(lang_list)

        texts = []
        for j in range(len(images)):
            caption = captions_dict[lang][j]
            texts.append(caption if caption is not None else "")

        # Forward pass
        image_embeds = model.encode_image(images)
        text_embeds = model.encode_text(texts)
        loss = model.contrastive_loss(image_embeds, text_embeds)

        # Backward pass
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        batch_count += 1

        iteration_losses.append(loss.item())

        if (i + 1) % display_interval == 0:
            # clear_output(wait=True)
            plt.figure(figsize=(8,5))
            plt.plot(iteration_losses, label="Iteration Loss")
            plt.title(f"Epoch {epoch+1} Iteration Loss")
            plt.xlabel("Iteration")
            plt.ylabel("Loss")
            plt.grid(True)
            plt.legend()
            plt.show()

    avg_loss = running_loss / batch_count if batch_count > 0 else 0
    torch.save(model.state_dict(), save_path)
    print(f"Model parameters saved to {save_path }")
    print(f"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}")


torch.save(model.state_dict(), save_path)
print(f"Model parameters saved to {save_path}")

